{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from uuid import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from uuid import * \n",
    "from uuid import UUID\n",
    "import time_uuid \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "spark= SparkSession.builder.config('spark.jars.packages','com.datastax.spark:spark-cassandra-connector_2.12:3.1.0').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Đọc data từ Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table = 'tracking',keyspace = 'study_de').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Xử lý custom_track = click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_clicks(df):\n",
    "    clicks_data = df.filter(df.custom_track == 'click')\n",
    "    clicks_data = clicks_data.na.fill({'bid':0})\n",
    "    clicks_data = clicks_data.na.fill({'job_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'publisher_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'group_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'campaign_id':0})\n",
    "    clicks_data.registerTempTable('clicks')\n",
    "    clicks_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , avg(bid) as bid_set, count(*) as clicks , sum(bid) as spend_hour from clicks\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return clicks_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Xử lý custom_track = conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_conversion(df):\n",
    "    conversion_data = df.filter(df.custom_track == 'conversion')\n",
    "    conversion_data = conversion_data.na.fill({'job_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'publisher_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'group_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'campaign_id':0})\n",
    "    conversion_data.registerTempTable('conversion')\n",
    "    conversion_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as conversions  from conversion\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return conversion_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Xử lý custom_track = qualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_qualified(df):    \n",
    "    qualified_data = df.filter(df.custom_track == 'qualified')\n",
    "    qualified_data = qualified_data.na.fill({'job_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'publisher_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'group_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'campaign_id':0})\n",
    "    qualified_data.registerTempTable('qualified')\n",
    "    qualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as qualified  from qualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return qualified_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Xử lý custom_track = unqualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_unqualified(df):\n",
    "    unqualified_data = df.filter(df.custom_track == 'unqualified')\n",
    "    unqualified_data = unqualified_data.na.fill({'job_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'publisher_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'group_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'campaign_id':0})\n",
    "    unqualified_data.registerTempTable('unqualified')\n",
    "    unqualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as unqualified  from unqualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return unqualified_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Join tất cả các kết quả xử lý để nhận được thông tin full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output):\n",
    "    final_data = clicks_output.join(conversion_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(qualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(unqualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full')\n",
    "    return final_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Tổng hợp output để lấy kết quả cuối cùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cassandra_data(df):\n",
    "    clicks_output = calculating_clicks(df)\n",
    "    conversion_output = calculating_conversion(df)\n",
    "    qualified_output = calculating_qualified(df)\n",
    "    unqualified_output = calculating_unqualified(df)\n",
    "    final_data = process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Lấy id của company merge vào output trên để lấy được id của company cho job đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_company_data(url,driver,user,password):\n",
    "    sql = \"\"\"(SELECT id as job_id, company_id, group_id, campaign_id FROM job) test\"\"\"\n",
    "    company = spark.read.format('jdbc').options(url=url, driver=driver, dbtable=sql, user=user, password=password).load()\n",
    "    return company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_output(df):\n",
    "    cassandra_output = process_cassandra_data(df)\n",
    "    company = retrieve_company_data(url = \"jdbc:mysql://localhost:3306/data_engineering\",driver = \"com.mysql.cj.jdbc.Driver\",user = 'root',password = '1') \n",
    "    final_output = cassandra_output.join(company,'job_id','left').drop(company.group_id).drop(company.campaign_id)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Bắn final output trên vào Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = '3.0.3'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.0.1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"kafka-ETL\")\\\n",
    "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Đọc lại data từ Cassandra rồi lấy hàm xử lý output đê xử lý data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df = spark.readStream \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"tracking\", keyspace=\"study_de\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df = cassandra_df.select('ts','job_id','custom_track','bid','campaign_id','group_id','publisher_id')\n",
    "cassandra_df = cassandra_df.filter(cassandra_df.job_id.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Biến đổi data từ cassandra về dạng key:value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df = cassandra_df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data vào kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df.write.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"topic\", \"project_ETL\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Đọc data từ kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_read = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"project_ETL\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = kf_read.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Lọc ra giá trị khác null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_data.select('value').filter(final_data.key.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Biến đổi data từ kafka về lại dataframe để đúng với cấu trúc bảng load vào MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = StructType([StructField('company_id', StringType(), True),\n",
    "                      StructField('job_id', StringType(), True),\n",
    "                      StructField('dates', StringType(), True),\n",
    "                      StructField('hours', StringType(), True),\n",
    "                      StructField('publisher_id', StringType(), True),\n",
    "                      StructField('campaign_id', StringType(), True),\n",
    "                      StructField('group_id', StringType(), True),\n",
    "                      StructField('bid_set', StringType(), True),\n",
    "                      StructField('clicks', StringType(), True),\n",
    "                      StructField('conversion', StringType(), True),\n",
    "                      StructField('qualified_application', StringType(), True),\n",
    "                      StructField('disqualified_application', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_kafka = result.withColumn('c1', F.from_json('value', schema = columns)).select('c1.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load Data vào mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_kafka.write.format('jdbc').option('url','jdbc:mysql://localhost:3306/data_engineering')\\\n",
    "                                     .option('driver','com.mysql.cj.jdbc.Driver')\\\n",
    "                                     .option('dbtable','event_DW')\\\n",
    "                                     .option('user','root')\\\n",
    "                                     .option('password','1').mode('append').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
